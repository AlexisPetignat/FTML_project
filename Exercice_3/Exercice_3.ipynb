{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b30b8f4e-9a22-4897-9b9a-cacbefeef210",
   "metadata": {},
   "source": [
    "# FTML Project - Exercice 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e9875-00d3-4447-b3c5-eaef56283a17",
   "metadata": {},
   "source": [
    "We recall the following notations and settings :\n",
    "\n",
    "- $\\epsilon$, vector of centered Gaussian noise with variance matrix $\\sigma^2I_n.$\n",
    "- $X = \\mathbb{R}^d$, input space\n",
    "- $\\mathcal{Y} = \\mathbb{R}$, output space\n",
    "\n",
    "The dataset is stored in the **design matrix** $\\mathcal{X} \\in \\mathbb{R}^{n \\times d}$.\n",
    "\n",
    "$$ \n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "x_1^T \\\\ \\ldots \\\\ x_i^T \\\\ \\ldots \\\\ x_n^T\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "x_{11} & \\ldots & x_{1j} & \\ldots & x_{1d} \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{i1} & \\ldots & x_{ij} & \\ldots & x_{id} \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n1} & \\ldots & x_{nj} & \\ldots & x_{nd} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We note $\\Vert\\cdot\\Vert = \\Vert\\cdot\\Vert_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca3bdbd-7b5c-4029-ae1b-003c245f835b",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "In the context of Proposition 1, we are given the expected value of the fixed design risk for the ordinary least squares (OLS) estimator \\(\\hat{\\theta}\\):\n",
    "\n",
    "$$ E\\left[R_{X}(\\hat{\\theta})\\right] = \\frac{n-d}{n} \\sigma^2 $$\n",
    "\n",
    "To compare this with the Bayes risk, we need to understand what the Bayes risk represents. The Bayes risk is the minimal expected risk achievable by any estimator under the given loss function, which, in the case of squared error loss, is the expected value of the variance of the noise \\(\\epsilon\\), i.e., \\(\\sigma^2\\).\n",
    "\n",
    "The Bayes risk is essentially the lowest possible risk that can be achieved by an estimator that knows the true underlying distribution of the data. It serves as a theoretical lower bound on the performance of any estimator.\n",
    "\n",
    "Comparing the two:\n",
    "\n",
    "- The value from Proposition 1 is $\\frac{n-d}{n} \\sigma^2$.\n",
    "- The Bayes risk is $\\sigma^2$.\n",
    "\n",
    "Since $\\frac{n-d}{n} < 1$ for $d > 0$, it follows that $\\frac{n-d}{n} \\sigma^2 < \\sigma^2$. Therefore, the value in Proposition 1 is smaller than the Bayes risk.\n",
    "\n",
    "### Interpretation and Discussion:\n",
    "\n",
    "- The result from Proposition 1 suggests that the expected risk of the OLS estimator is always less than the Bayes risk when $d > 0$. This might seem counterintuitive at first because the Bayes risk is supposed to be the minimal achievable risk.\n",
    "- The key insight here is that the Bayes risk is a theoretical minimum assuming perfect knowledge of the underlying data distribution, whereas the OLS estimator is derived from observed data and does not assume such perfect knowledge.\n",
    "- As $n$ increases, the term $\\frac{n-d}{n}$ approaches 1, meaning the expected risk of the OLS estimator approaches the Bayes risk. This reflects the fact that with more data, the OLS estimator can better approximate the true underlying model.\n",
    "- For a fixed $n$, as $d$ increases, the term $\\frac{n-d}{n}$ decreases, indicating that the expected risk of the OLS estimator decreases. However, this also implies that the model complexity is increasing, which could lead to overfitting if not managed properly.\n",
    "\n",
    "In summary, the result from Proposition 1 highlights the trade-off between model complexity and the amount of data available, showing how the expected risk of the OLS estimator relates to the theoretical minimum risk represented by the Bayes risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d4c581-0331-4ed7-8862-fb0325bd765d",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "We aim to show that :\n",
    "\n",
    "$$\n",
    "E\\left[R_n(\\hat{\\theta})\\right] = E_{\\epsilon}\\left[\\frac{1}{n}\\Vert(I_n - X(X^TX)^{-1}X^T)\\epsilon\\Vert^2\\right]\n",
    "$$\n",
    "\n",
    "where $E_\\epsilon$ means that the expected value is over $\\epsilon$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E\\left[R_n(\\hat{\\theta})\\right] &= E\\left[\\frac{1}{n}\\sum_{i=1}^n(y_i - \\hat{\\theta}^T x_i)^2\\right] \\\\\n",
    "&= E\\left[\\frac{1}{n}\\Vert y - X\\hat{\\theta}\\Vert^2\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We recall that, given $X$ and $y$, the ordinary least squared estimator, that minimizes the empirical risk is defined as :\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E\\left[R_n(\\hat{\\theta})\\right] &= E\\left[\\frac{1}{n}\\Vert y - X(X^TX)^{-1}X^Ty\\Vert^2\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In the **linear model**, we assume that\n",
    "\n",
    "$$\n",
    "y = X\\hat{\\theta} + \\epsilon\n",
    "$$\n",
    "\n",
    "Therefore (and we can directly write the expected value according to $\\epsilon$ because it is the only source of randomness here),\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E\\left[R_n(\\hat{\\theta})\\right] &= E_{\\epsilon}\\left[\\frac{1}{n}\\Vert X\\hat{\\theta} + \\epsilon - X(X^TX)^{-1}X^T(X\\hat{\\theta} + \\epsilon)\\Vert^2\\right] \\\\\n",
    "&= E_{\\epsilon}\\left[\\frac{1}{n}\\Vert X \\hat{\\theta} + \\epsilon - X(X^T X)^{-1} X^T X \\hat{\\theta} - X(X^T X)^{-1} X^T \\epsilon \\Vert^2\\right] \\\\\n",
    "&= E_{\\epsilon}\\left[\\frac{1}{n}\\Vert (I_n - X(X^T X)^{-1} X^T) \\epsilon + X \\hat{\\theta} - X(X^T X)^{-1} X^T X \\hat{\\theta} \\Vert^2\\right] \\\\\n",
    "&= E_{\\epsilon}\\left[\\frac{1}{n}\\Vert (I_n - X(X^T X)^{-1} X^T) \\epsilon + X \\hat{\\theta} - X\\hat{\\theta} \\Vert^2\\right] \\\\\n",
    "&= E_{\\epsilon}\\left[\\frac{1}{n}\\Vert (I_n - X(X^T X)^{-1} X^T) \\epsilon \\Vert^2\\right]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f18d8-6b50-4964-92bd-11f8c3190444",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{n,n}$. We will show that :\n",
    "\n",
    "$$\\sum_{(i,j) \\in [1,n]^2} A_{ij}^2 = \\text{tr}(A^T A)$$\n",
    "\n",
    "The trace of a matrix, denoted as $\\text{tr}(A)$, is the sum of the diagonal elements of the matrix $A$.\n",
    "\n",
    "Let's consider the product $A^T A$. If $A$ is an $n \\times n$ matrix, then $A^T A$ is also an $n \\times n$ matrix.\n",
    "\n",
    "We recall that the elements of the matrix $A^T A$ are given by:\n",
    "\n",
    "$$ (A^T A)_{ij} = \\sum_{k=1}^{n} A_{ki} A_{kj} $$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$ \\text{tr}(A^T A) = \\sum_{i=1}^{n} (A^T A)_{ii} = \\sum_{i=1}^{n} \\sum_{k=1}^{n} A_{ki}^2 $$\n",
    "\n",
    "We can reindex the sum to obtain:\n",
    "\n",
    "$$ \\text{tr}(A^T A) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} A_{ij}^2 $$\n",
    "\n",
    "Thus, we have shown that:\n",
    "\n",
    "$$ \\sum_{(i,j) \\in [1,n]^2} A_{ij}^2 = \\text{tr}(A^T A) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba335e85-903e-46dd-9b6c-860295aedbe8",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Let us show that :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E_{\\epsilon} \\left[ \\frac{1}{n} \\Vert A \\epsilon \\Vert^{2} \\right] &= \\frac{\\sigma^{2}}{n} tr(A^{T}A)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E_{\\epsilon} \\left[ \\frac{1}{n} \\Vert A \\epsilon \\Vert^{2} \\right] &= \\frac{1}{n} E_{\\epsilon} \\left[ \\Vert A \\epsilon \\Vert^{2} \\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now,\n",
    "\n",
    "$$\n",
    "\\Vert A \\epsilon \\Vert^2 = (A \\epsilon)^T (A \\epsilon) = \\epsilon^T A^T A \\epsilon\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "E_\\epsilon \\left[ \\frac{1}{n} \\Vert A \\epsilon \\Vert^2 \\right] = \\frac{1}{n} E_\\epsilon \\left[ \\epsilon^T A^T A \\epsilon \\right]\n",
    "$$\n",
    "\n",
    "But, for a random vector $\\epsilon$ centered and of covariance $\\Sigma$ :\n",
    "\n",
    "$$\n",
    "B \\left[ \\epsilon^T B \\epsilon \\right] = tr(B \\Sigma)\n",
    "$$\n",
    "\n",
    "Here, $\\Sigma = \\sigma^2 I$ so :\n",
    "\n",
    "$$\n",
    "E_\\epsilon \\left[ \\epsilon^T A^T A \\epsilon \\right] = tr(A^T A \\cdot \\sigma^2 I) = \\sigma^2 tr(A^T A)\n",
    "$$\n",
    "\n",
    "So :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E_{\\epsilon} \\left[ \\frac{1}{n} \\Vert A \\epsilon \\Vert^{2} \\right] &= \\frac{1}{n} \\cdot \\sigma^2 \\cdot tr(A^T A) \\\\\n",
    "&= \\frac{\\sigma^2}{n} tr(A^T A)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034169e7-4757-4bbb-9305-7a46754bcb64",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "We recall that $$A = I_n - X(X^T X)^{-1} X^T$$\n",
    "\n",
    "Let us show that $A^T A = A$ :\n",
    "\n",
    "First, we note that $A$ is symetrical, indeed :\n",
    "\n",
    "$$\n",
    "A^T = (I_n - X(X^TX)^{-1}X^T)^T = I_n - X(X^TX)^{-1}X^T = A\n",
    "$$\n",
    "\n",
    "Futhermore, $A$ is idempotent :\n",
    "\n",
    "$$\n",
    "A^2 = (I_n - X(X^TX)^{-1}X^T)^2\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A^TA &= (I_n - X(X^TX)^{-1}X^T)^2 \\\\\n",
    "&= I_n - 2X(X^TX)^{-1}X^T + X(X^TX)^{-1}X^TX(X^TX)^{-1}X^T \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "But $X^TX$ is invertible, so $X^TX(X^TX)^{-1} = I$. Therefore,\n",
    "\n",
    "$$\n",
    "X^TX(X^TX)^{-1} = I \\implies X^TX(X^TX)^{-1}X^T = X^T\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X(X^TX)^{-1}X^TX(X^TX)^{-1}X^T &= X(X^TX)^{-1}(X^TX)(X^TX)^{-1}X^T \\\\\n",
    "&= X(X^TX)^{-1}X^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Finally,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A^2 &= A^TA \\\\\n",
    "&= I_n - 2X(X^TX)^{-1}X^T + X(X^TX)^{-1}X^T \\\\\n",
    "&= A\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Which is the expected result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b596c77-0b12-4b2a-9df9-a674dfcf191e",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "We want to show the Proposition 1. e.g that in the linear model, fixed design we have :\n",
    "\n",
    "$$\n",
    "E\\left[R_X(\\hat{\\theta})\\right] = \\frac{n - d}{n} \\sigma^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E \\left[ R_X(\\hat{\\theta}) \\right] &= E_\\epsilon \\left[ \\frac{1}{n} \\Vert (I_n - X(X^T X)^{-1} X^T) \\epsilon \\Vert^2 \\right] \\quad \\text{(according to question 2)} \\\\\n",
    "&= E_\\epsilon \\left[ \\frac{1}{n} \\Vert A \\epsilon \\Vert^2 \\right] \\quad \\text{with} \\quad A = I_n - X(X^T X)^{-1} X^T \\\\\n",
    "&= \\frac{\\sigma^2}{n} tr(A^TA) \\quad \\text{(from question 4)} \\\\\n",
    "&= \\frac{tr(I_n - X(X^T X)^{-1} X^T)}{n} \\sigma^2 \\quad \\text{(we show it in question 5} \\\\\n",
    "&= \\frac{tr(I_n) - tr(X(X^T X)^{-1} X^T)}{n} \\sigma^2 \\\\\n",
    "&= \\frac{n - tr(I_d)}{n} \\sigma^2 \\\\\n",
    "&= \\frac{n - d}{n} \\sigma^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We demonstrate Proposition 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d864de6-ee21-4158-a260-35f554342bb4",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "We aim to find the expected value of\n",
    "\n",
    "$$\n",
    "\\frac{\\Vert y - X \\hat{\\theta} \\Vert^2}{n - d}\n",
    "$$\n",
    "\n",
    "First, we recall that :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "R_n(\\hat{\\theta}) &= \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{\\theta}x_i)^2 \\\\\n",
    "&= \\frac{1}{n} \\Vert y - X \\hat{\\theta} \\Vert^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E \\left[ \\frac{\\Vert y - X \\hat{\\theta} \\Vert^2}{n - d} \\right] &= E \\left[ \\frac{nR_n(\\hat{\\theta})}{n - d} \\right] \\\\\n",
    "&= \\frac{n}{n - d} E \\left[ R_n(\\hat{\\theta}) \\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "From Propositon 1. we know that :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E \\left[ R_X(\\hat{\\theta}) \\right] &= \\frac{n - d}{n} \\sigma^2 \\\\\n",
    "\\Leftrightarrow \\frac{\\sigma^2}{E \\left[ R_X(\\hat{\\theta}) \\right]} &= \\frac{n}{n - d}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E \\left[ \\frac{\\Vert y - X \\hat{\\theta} \\Vert^2}{n - d} \\right] &= \\frac{\\sigma^2}{E \\left[ R_X(\\hat{\\theta}) \\right]} \\cdot E \\left[ R_X(\\hat{\\theta}) \\right] \\\\\n",
    "&= \\sigma^2\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76557aed-a5dc-4770-b092-c348012f499e",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f72e045-0dcd-4c15-bab6-b06127b19b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de8119fb-fe3b-4ee3-a31f-3c1b3670e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLS_estimator(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    covariance_matrix = X.T @ X\n",
    "    inverse_covariance = np.linalg.inv(covariance_matrix)\n",
    "    theta_hat = inverse_covariance @ (X.T @ y)\n",
    "    return theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b997cb5-5106-433b-b324-6865d7171348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output_data(\n",
    "    X: np.ndarray, theta_star: np.ndarray, sigma: float, rng, n_repetitions: int\n",
    ") -> np.ndarray:\n",
    "    n = X.shape[0]\n",
    "    noise = rng.normal(0, sigma, size=(n, n_repetitions))\n",
    "    y = X @ theta_star + noise\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b24a452a-a481-4d86-8801-6754c0089c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_error(sigma: int, n_train: int, d: int, n_repetitions: int) -> float:\n",
    "    rng = np.random.default_rng()\n",
    "    \n",
    "    X = rng.uniform(low=0, high=1, size=(n_train, d))\n",
    "    \n",
    "    theta_star = rng.uniform(low=0, high=1, size=(d, 1))\n",
    "    \n",
    "    y = generate_output_data(\n",
    "        X=X,\n",
    "        theta_star=theta_star,\n",
    "        sigma=sigma,\n",
    "        rng=rng,\n",
    "        n_repetitions=n_repetitions,\n",
    "    )\n",
    "    \n",
    "    theta_hat = OLS_estimator(\n",
    "        X=X,\n",
    "        y=y,\n",
    "    )\n",
    "\n",
    "    mean_test_error = ((np.linalg.norm(y - (X @ theta_hat))**2) / (n_train - d)) / n_repetitions\n",
    "    return mean_test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce8e9bfa-2c5f-4f2a-af26-24c458096dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 8, sigma^2 = 64, E = 63.99\n"
     ]
    }
   ],
   "source": [
    "sigma = 8\n",
    "with np.errstate(all=\"ignore\"): # Because of https://github.com/numpy/numpy/issues/28687 \n",
    "    mean_test_error = test_error(sigma, 200, 30, 100000)\n",
    "print(f'sigma = {sigma}, sigma^2 = {sigma**2}, E = {mean_test_error:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e0539f-03b8-4947-9c5a-d18dcb92a643",
   "metadata": {},
   "source": [
    "We find around 64 for the expected value, which was the chosen one at the beginning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13",
   "language": "python",
   "name": "py313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
