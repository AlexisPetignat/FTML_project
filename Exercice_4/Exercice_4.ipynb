{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b30b8f4e-9a22-4897-9b9a-cacbefeef210",
   "metadata": {},
   "source": [
    "# FTML Project Exercice 4\n",
    "\n",
    "For this exercice, we will try to run a regression analysis on the given dataset using several methods.\n",
    "\n",
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f6bb1459-6658-4783-814e-d24afa2393a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    AdaBoostRegressor,\n",
    "    BaggingRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    HistGradientBoostingRegressor,\n",
    ")\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.load(\"../data/regression/X_train.npy\")\n",
    "X_test = np.load(\"../data/regression/X_test.npy\")\n",
    "Y_train = np.load(\"../data/regression/y_train.npy\")\n",
    "Y_test = np.load(\"../data/regression/y_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a1d5b0-2762-47e8-a838-b5f0f650b7d9",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "\n",
    "Several preprocessing steps are required in order to exploit the data. We will reformat the Y_test and Y_train arrays, since they are of wrong size, by squeezing them. We will also reduce the dimentionality of the dataset since it is quite massive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eadd2053-8c2f-4c76-bbe3-6e59f0155bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze output arrays\n",
    "Y_train = np.squeeze(Y_train)\n",
    "Y_test = np.squeeze(Y_test)\n",
    "\n",
    "# Reduce dimension\n",
    "dim = X_train.shape[1] / 5\n",
    "pca = PCA(n_components=int(dim))\n",
    "\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed424c-cfa0-4219-bb64-c03d0b80eb71",
   "metadata": {},
   "source": [
    "## Run the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a078c1f0-b4fd-4c66-bf3d-d5f63ee2449a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.22140271835518\n"
     ]
    }
   ],
   "source": [
    "polynomial = make_pipeline(PolynomialFeatures(3, include_bias=False))\n",
    "\n",
    "model = make_pipeline(\n",
    "        polynomial,\n",
    "        GradientBoostingRegressor(),\n",
    "    )\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562efccf-3188-4728-a54c-2d497a980e88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ftml-venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
