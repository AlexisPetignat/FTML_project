{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b30b8f4e-9a22-4897-9b9a-cacbefeef210",
   "metadata": {},
   "source": [
    "# FTML Project Exercice 4\n",
    "\n",
    "For this exercice, we will try to run a regression analysis on the given dataset using several methods.\n",
    "\n",
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6bb1459-6658-4783-814e-d24afa2393a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    AdaBoostRegressor,\n",
    "    BaggingRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    HistGradientBoostingRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    StackingRegressor,\n",
    "    VotingRegressor\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import (\n",
    "    Ridge,\n",
    "    LinearRegression,\n",
    "    ARDRegression\n",
    ")\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.load(\"../data/regression/X_train.npy\")\n",
    "X_test = np.load(\"../data/regression/X_test.npy\")\n",
    "Y_train = np.load(\"../data/regression/y_train.npy\")\n",
    "Y_test = np.load(\"../data/regression/y_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a1d5b0-2762-47e8-a838-b5f0f650b7d9",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "\n",
    "A slight preprocessing step is required in order to exploit the data. We will reformat the Y_test and Y_train arrays, since they are of wrong size, by squeezing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd2053-8c2f-4c76-bbe3-6e59f0155bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze output arrays\n",
    "Y_train = np.squeeze(Y_train)\n",
    "Y_test = np.squeeze(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed424c-cfa0-4219-bb64-c03d0b80eb71",
   "metadata": {},
   "source": [
    "## Run the analysis\n",
    "\n",
    "The following function will run the regression analysis of a model on the dataset and print the r2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a078c1f0-b4fd-4c66-bf3d-d5f63ee2449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#polynomial = make_pipeline(PolynomialFeatures(2, include_bias=False))\n",
    "def try_model(regressor) :\n",
    "    model = make_pipeline(\n",
    "            regressor,\n",
    "        )\n",
    "    \n",
    "    model.fit(X_train, Y_train.ravel())\n",
    "    \n",
    "    Y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(Y_test, Y_pred)\n",
    "    print(f\"Score for regressor {regressor}: {r2}\")\n",
    "\n",
    "def try_model_scaled(regressor) :\n",
    "    model = make_pipeline(\n",
    "            StandardScaler(),\n",
    "            regressor,\n",
    "        )\n",
    "    \n",
    "    model.fit(X_train, Y_train.ravel())\n",
    "    \n",
    "    Y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(Y_test, Y_pred)\n",
    "    print(f\"Score for regressor {regressor} (scaled): {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a5e658-04b1-46f9-bdcf-0321cc850d83",
   "metadata": {},
   "source": [
    "Machine Learning is by definition a matter of exploration to find the best method to comprehend a dataset. This is why we chose to take our first step into it by running several regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562efccf-3188-4728-a54c-2d497a980e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = [\n",
    "    RandomForestRegressor(),\n",
    "    AdaBoostRegressor(),\n",
    "    BaggingRegressor(),\n",
    "    GradientBoostingRegressor(),\n",
    "    HistGradientBoostingRegressor(),\n",
    "    ExtraTreesRegressor(),\n",
    "    Ridge(),\n",
    "    LinearRegression(),\n",
    "    ARDRegression()\n",
    "]\n",
    "\n",
    "for regressor in regressors :\n",
    "    try_model(regressor)\n",
    "    try_model_scaled(regressor)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee72a24-8c52-451d-8164-b51e148824cf",
   "metadata": {},
   "source": [
    "## Observation of the results\n",
    "\n",
    "The scores are very spread out. The linear regression is by far the worst, whereas the ARD regression performs very well, even without optimisation on both scaled and unscaled data, even approaching the bayes estimator!\n",
    "\n",
    "Scaling the data does not make much difference and tends to worsen the scores, except for ARD which improves the result by 0,015.\n",
    "\n",
    "## Optimisation\n",
    "\n",
    "Now we will try to refine the models even more by running an optimisation, using Optuna. We will not detail what is optuna here, but we chose it because it allows one to easily optimize the hyperparameters of a model in very few lines of code. \n",
    "We will only take the three best models, which are ARDR, Ridge and HistGradientBoosting, and scale the data only for ARD since it is the only one that benefited from it.\n",
    "\n",
    "### HistGradient optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ca6f56-7049-4187-ac8f-9fa9f9ebb73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'max_iter': trial.suggest_int('max_iter', 200, 1000),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 31, 255),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 10, 100),\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization', 1e-4, 10.0, log=True),\n",
    "        'max_bins': trial.suggest_int('max_bins', 64, 255),\n",
    "        'early_stopping': False\n",
    "    }\n",
    "    \n",
    "    \n",
    "    model = make_pipeline(HistGradientBoostingRegressor(**params))\n",
    "    model.fit(X_train, Y_train.ravel())\n",
    "    Y_pred = model.predict(X_test)\n",
    "    return r2_score(Y_test, Y_pred)\n",
    "\n",
    "hist_study = optuna.create_study(study_name=\"HistGradient hyperparameter optimisation\", direction=\"maximize\")\n",
    "hist_study.optimize(objective, n_trials=300)\n",
    "\n",
    "hist_study.best_params, hist_study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66120adb-dfc6-4483-bdeb-6a543cb92b8b",
   "metadata": {},
   "source": [
    "### Ridge Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ddeded-4c79-49cd-a9d8-71ba50f734ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'alpha': trial.suggest_float('alpha', 1e-4, 100.0, log=True),\n",
    "        'solver': trial.suggest_categorical('solver', ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'saga']),\n",
    "        'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False])\n",
    "    }\n",
    "    \n",
    "    model = make_pipeline(Ridge(**params))\n",
    "    model.fit(X_train, Y_train.ravel())\n",
    "    Y_pred = model.predict(X_test)\n",
    "    return r2_score(Y_test, Y_pred)\n",
    "\n",
    "ridge_study = optuna.create_study(study_name=\"Ridge hyperparameter optimisation\", direction=\"maximize\")\n",
    "ridge_study.optimize(objective, n_trials=300)\n",
    "\n",
    "ridge_study.best_params, ridge_study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a39a645-3843-43d5-91c2-7b5b0d31487e",
   "metadata": {},
   "source": [
    "### ARD optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0454fbd-d877-42b5-bfec-462126e6e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'alpha_1': trial.suggest_float('alpha_1', 1e-7, 1e-3, log=True),\n",
    "        'alpha_2': trial.suggest_float('alpha_2', 1e-7, 1e-3, log=True),\n",
    "        'lambda_1': trial.suggest_float('lambda_1', 1e-7, 1e-3, log=True),\n",
    "        'lambda_2': trial.suggest_float('lambda_2', 1e-7, 1e-3, log=True),\n",
    "        'threshold_lambda': trial.suggest_float('threshold_lambda', 1000.0, 10000.0),\n",
    "        'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False])\n",
    "    }\n",
    "    \n",
    "    model = make_pipeline(StandardScaler(), ARDRegression(**params))\n",
    "    model.fit(X_train, Y_train.ravel())\n",
    "    Y_pred = model.predict(X_test)\n",
    "    return r2_score(Y_test, Y_pred)\n",
    "\n",
    "ard_study = optuna.create_study(study_name=\"Ridge hyperparameter optimisation\", direction=\"maximize\")\n",
    "ard_study.optimize(objective, n_trials=300)\n",
    "\n",
    "ard_study.best_params, ard_study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ae4be5-7e17-4205-a2bb-0611ddafc189",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Machine Learning is a very experimental process. A lot of trial and error, exploration and guesses are required. After running several regression models, we ended up choosing 3 good ones to optimize.\n",
    "After optimisation, we end up with 2 good models and an excellent one performing better than the Bayes estimator at a glorious 0.93 r2_score! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
